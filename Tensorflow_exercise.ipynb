{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current w1 is [[-0.81031823]\n",
      " [ 1.4855988 ]]\n",
      "current w1 is [[-0.7132425]\n",
      " [ 1.5827229]]\n",
      "current w1 is [[-0.61900544]\n",
      " [ 1.6698811 ]]\n",
      "current w1 is [[-0.5268384]\n",
      " [ 1.7513007]]\n",
      "current w1 is [[-0.4349928]\n",
      " [ 1.8327824]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "#4.2..2 \n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(None,2),name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1),name=\"y-input\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))\n",
    "y = tf.matmul(x,w1)\n",
    "\n",
    "loss_less = 10\n",
    "loss_more = 1\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*loss_more,(y_-y)*loss_less))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "rdm = RandomState(1)\n",
    "\n",
    "dataset_size = 128\n",
    "\n",
    "X = rdm.rand(dataset_size,2)\n",
    "Y = [[x1+x2+rdm.rand()/10.0-0.05] for (x1,x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    Steps = 500\n",
    "    for i in range(Steps):\n",
    "        start = (i*batch_size) % dataset_size\n",
    "        end = min(start+batch_size,dataset_size)\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "        \n",
    "        if i% 100==0:\n",
    "            print(\"current w1 is {}\".format(sess.run(w1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#4.4.2\n",
    "\n",
    "def get_weight(shape,lambd):\n",
    "    var = tf.Variable(tf.random_normal(shape),dtype=tf.float32)\n",
    "    tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(lambd)(var))\n",
    "    return var\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(None,2))\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "layer_dimension = [2,10,10,10,2]\n",
    "n_layer = len(layer_dimension)\n",
    "cur_layer = x\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "for i in range(1,n_layer):\n",
    "    out_dimension = layer_dimension[i]\n",
    "    weight = get_weight([in_dimension,out_dimension],0.001)\n",
    "    bias = tf.Variable(tf.constant(0.1,shape=[out_dimension]))\n",
    "    cur_layer = tf.nn.relu(tf.matmul(cur_layer,weight)+bias)\n",
    "    in_dimension = layer_dimension[i]\n",
    "    \n",
    "mes_loss = tf.reduce_mean(tf.square(y_ - cur_layer))\n",
    "tf.add_to_collection('losses',mes_loss)\n",
    "\n",
    "loss = tf.add_n(tf.get_collection('losses'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /path/to/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting /path/to/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /path/to/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /path/to/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "After 0 training steps, validation accuracy using average model is 0.0802\n",
      "After 100 training steps, validation accuracy using average model is 0.9416\n",
      "After 200 training steps, validation accuracy using average model is 0.957\n",
      "After 300 training steps, validation accuracy using average model is 0.9646\n",
      "After 400 training steps, validation accuracy using average model is 0.9694\n",
      "After 500 training steps, validation accuracy using average model is 0.9712\n",
      "After 600 training steps, validation accuracy using average model is 0.974\n",
      "After 700 training steps, validation accuracy using average model is 0.9746\n",
      "After 800 training steps, validation accuracy using average model is 0.9754\n",
      "After 900 training steps, validation accuracy using average model is 0.9758\n",
      "After 1000 training steps, validation accuracy using average model is 0.978\n",
      "After 1100 training steps, validation accuracy using average model is 0.9774\n",
      "After 1200 training steps, validation accuracy using average model is 0.978\n",
      "After 1300 training steps, validation accuracy using average model is 0.9786\n",
      "After 1400 training steps, validation accuracy using average model is 0.9808\n",
      "After 1500 training steps, validation accuracy using average model is 0.981\n",
      "After 1600 training steps, validation accuracy using average model is 0.9804\n",
      "After 1700 training steps, validation accuracy using average model is 0.9808\n",
      "After 1800 training steps, validation accuracy using average model is 0.9816\n",
      "After 1900 training steps, validation accuracy using average model is 0.9814\n",
      "After 2000 training steps, validation accuracy using average model is 0.9824\n",
      "After 2100 training steps, validation accuracy using average model is 0.9824\n",
      "After 2200 training steps, validation accuracy using average model is 0.9822\n",
      "After 2300 training steps, validation accuracy using average model is 0.9822\n",
      "After 2400 training steps, validation accuracy using average model is 0.9818\n",
      "After 2500 training steps, validation accuracy using average model is 0.9812\n",
      "After 2600 training steps, validation accuracy using average model is 0.9812\n",
      "After 2700 training steps, validation accuracy using average model is 0.9824\n",
      "After 2800 training steps, validation accuracy using average model is 0.9838\n",
      "After 2900 training steps, validation accuracy using average model is 0.9844\n",
      "After 3000 training steps, validation accuracy using average model is 0.9832\n",
      "After 3100 training steps, validation accuracy using average model is 0.985\n",
      "After 3200 training steps, validation accuracy using average model is 0.9848\n",
      "After 3300 training steps, validation accuracy using average model is 0.9848\n",
      "After 3400 training steps, validation accuracy using average model is 0.9836\n",
      "After 3500 training steps, validation accuracy using average model is 0.9838\n",
      "After 3600 training steps, validation accuracy using average model is 0.9836\n",
      "After 3700 training steps, validation accuracy using average model is 0.9844\n",
      "After 3800 training steps, validation accuracy using average model is 0.9844\n",
      "After 3900 training steps, validation accuracy using average model is 0.9846\n",
      "After 4000 training steps, validation accuracy using average model is 0.9846\n",
      "After 4100 training steps, validation accuracy using average model is 0.9838\n",
      "After 4200 training steps, validation accuracy using average model is 0.9832\n",
      "After 4300 training steps, validation accuracy using average model is 0.984\n",
      "After 4400 training steps, validation accuracy using average model is 0.9836\n",
      "After 4500 training steps, validation accuracy using average model is 0.9834\n",
      "After 4600 training steps, validation accuracy using average model is 0.983\n",
      "After 4700 training steps, validation accuracy using average model is 0.9828\n",
      "After 4800 training steps, validation accuracy using average model is 0.9836\n",
      "After 4900 training steps, validation accuracy using average model is 0.984\n",
      "After 5000 training steps, Test accuracy using average model is 0.9833\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\virtualenv_jupyter_conda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#5.1 5.2\n",
    "\n",
    "#MNIST 常数\n",
    "INPUT_NODE = 784 #输入节点数，即像素点个数\n",
    "OUTPUT_NODE = 10 #输出点个数，0-9共10个\n",
    "\n",
    "LAYER1_NODE = 500 #一层隐藏层，共500节点\n",
    "BATCH_SIZE = 100 #一个训练Batch个数\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8 #基础学习率\n",
    "LEARNING_RATE_DECAY = 0.99 #学习率的衰减率\n",
    "REGULARIZATION_RATE = 0.0001 #正则化项的损失函数的系数\n",
    "TRAINING_STEPS = 5000 #训练轮数\n",
    "MOVING_AVERAGE_DECAY = 0.99 #滑动平均衰减率\n",
    "\n",
    "def inference(input_tensor,avg_class,weights1,bias1,weights2,bias2):\n",
    "    '''\n",
    "    计算前向传播结果，定义Relu函数，实现3层全连接的神经网络\n",
    "    input_tensor：输入的当前网络层的输入X或激活函数a\n",
    "    avg_class：为空时，使用当前值计算；avg_class不为空时，使用传入的参数进行计算\n",
    "    weights1：隐藏层的权重系数\n",
    "    bias1：隐藏层的偏置函数\n",
    "    weights2：输出层的权重系数\n",
    "    bias2：输出层的偏置函数\n",
    "    '''\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor,weights1)+bias1)\n",
    "        return tf.matmul(layer1,weights2)+bias2\n",
    "    else:\n",
    "        #avg_class计算滑动平均值\n",
    "        layer2 = tf.nn.relu(tf.matmul(input_tensor,avg_class.average(weights1))+avg_class.average(bias1))\n",
    "        return tf.matmul(layer2,avg_class.average(weights2))+avg_class.average(bias2)\n",
    "\n",
    "#模型训练\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32,[None,INPUT_NODE],name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE],name=\"y-input\")\n",
    "    \n",
    "    #隐藏层的参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev=0.1))\n",
    "    bias1 = tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    #输出层的参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE],stddev=0.1))\n",
    "    bias2 = tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    #计算当前参数下的前向传播结果\n",
    "    y = inference(x,None,weights1,bias1,weights2,bias2)\n",
    "    \n",
    "    \n",
    "    global_step = tf.Variable(0,trainable=False)  #训练轮数（一般将训练轮数的变量指定为不可训练的参数）\n",
    "    \n",
    "    #给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    \n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables()) #在所有代表神经网络参数的变量使用滑动平均，其他辅助变量不使用\n",
    "    \n",
    "    #使用滑动平均之后的前向传播结果\n",
    "    average_y = inference(x,variable_averages,weights1,bias1,weights2,bias2)\n",
    "    \n",
    "    #计算交叉熵作为预测值和真实值之间的差距作为损失函数，sparse_softmax_cross_entropy_with_logits（arg1,arg2），如果分类问题只有一个正确答案时，\n",
    "    #可以用这个函数加速交叉熵的计算\n",
    "    #arg1是不包括softmax层的前向传播结果，arg2是训练数据的正确答案，用tf.argmax(y_,1)得到正确答案的编号\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    \n",
    "    #计算当前Batch中所有样例的交叉熵均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    #计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1)+regularizer(weights2)\n",
    "    #总损失=交叉熵损失+正则化损失\n",
    "    loss = cross_entropy_mean+regularization\n",
    "    \n",
    "    #设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,  #基础学习率\n",
    "        global_step,         #当前迭代的轮数\n",
    "        mnist.train.num_examples/BATCH_SIZE, #过完所有的训练数据需要的迭代次数\n",
    "        LEARNING_RATE_DECAY) #学习率衰减速度\n",
    "    \n",
    "    #使用tf.train.GradientDescentOptimizer优化算法优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    \n",
    "    #反向传播更新神经网络的参数\n",
    "    with tf.control_dependencies([train_step,variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    #判断使用滑动平均模型的神经网络的前向传播结果是否正确\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))\n",
    "    \n",
    "    #先将Bool转化为实数，然后计算均值\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "    #初始化会话并训练\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        #准备数据\n",
    "        validate_feed = {x:mnist.validation.images,y_:mnist.validation.labels}\n",
    "        test_feed = {x:mnist.test.images,y_:mnist.test.labels}\n",
    "        \n",
    "        #迭代训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            #每100轮输出一次结果\n",
    "            if i%100 == 0:\n",
    "                validate_acc = sess.run(accuracy,feed_dict=validate_feed)\n",
    "                print(\"After %d training steps, validation accuracy using average model is %g\"%(i,validate_acc))\n",
    "            \n",
    "            #产生一轮的Batch训练数据，运行训练过程\n",
    "            xs,ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op,feed_dict={x:xs,y_:ys})\n",
    "            \n",
    "        #训练结束之后，在测试数据上检测正确率   \n",
    "        test_acc = sess.run(accuracy,feed_dict=test_feed)\n",
    "        print(\"After %d training steps, Test accuracy using average model is %g\"%(TRAINING_STEPS,test_acc))\n",
    "        \n",
    "def main(arvg=None):\n",
    "    mnist = input_data.read_data_sets(\"/path/to/MNIST_data\",one_hot = True)\n",
    "    train(mnist)\n",
    "\n",
    "#Tensorflow提供主程序入口，tf.app.run会调用main函数\n",
    "if  __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
